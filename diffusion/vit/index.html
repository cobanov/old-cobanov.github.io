<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Mert Cobanov" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>ViT Visual Transformers - Cobanov</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "ViT Visual Transformers";
        var mkdocs_page_input_path = "diffusion/vit.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Cobanov
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Blog</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../..">Hello ğŸ‘‹</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../helpers/">Helper one-liners</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../new-macbook/">New Macbook Installation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cyberpunk/">Cypherpunk Manifesto</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../python-conf/">Doing Python Configuration Right</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../windows-wsl-ssh/">Windows WSL Activate SSH</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../openpose/">OpenPose</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../nerf/">NeRF</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../instagram/">Instagram Non-followers</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Portfolio</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../portfolio/">Hey</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">About</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../about/">Main Page</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">diffusion-book</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../ldm/">What is Latent Diffusion</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">ViT Visual Transformers</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#tl-dr">TL; DR</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#vision-transformers">Vision Transformers</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#vits-vs-cnn">ViTs vs. CNN</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#attention-mekanizmasi">Attention Mekanizmasi</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#vit-implementasyonlari">ViT Implementasyonlari</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#mimari">Mimari</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#vit-mimarisi">ViT Mimarisi</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#patch-embeddings">Patch Embeddings</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#linear-projection-of-flattened-patches">Linear Projection of Flattened Patches</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#positional-embeddings">Positional embeddings</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#transformer-encoding">Transformer Encoding</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#toparlama">Toparlama</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#egitim">Egitim</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#usage">Usage</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#sources">Sources</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../glossary/">Glossary</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../samplers/">Samplers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../prompting/">Prompting</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../blender-depthmap/">Blender Depthmap</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../training-ldm/">Training LDM</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../fine-tuning/">Fine Tuning</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../dream-booth/">Dream Booth</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tools/">Tools & Blogs and Useful Links</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Cobanov</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>diffusion-book &raquo;</li>
      <li>ViT Visual Transformers</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="vision-transformers-vits">Vision Transformers ViTs</h1>
<h2 id="introduction">Introduction</h2>
<p>2022 yili yapay zekanin yili oldu, yapay zeka sanatinda, dogal dil islemede, goruntu islemede, ses teknolojilerinde inanilmaz gelismeler yasandi. Hem <a href="https://huggingface.co/">Huggingface</a> hem <a href="https://openai.com/">OpenAI</a> firtinalar kopardi. Onceki yillara nazaran bu yapay zeka teknolojileri hem demokratiklesti hem de son kullaniciya daha fazla ulasma firsati buldu.</p>
<p>Bugun sizinle bu gelismelerden biri olan Vision Transformerlardan bahsedecegim. Makale giderek derinlesip tekniklesiyor, bu yuzden tum akisi basitten karmasiga olacak sekilde siraladim. Bu konu ne kadar ilginizi cekiyorsa o kadar ileri gidebilirsiniz.</p>
<p><code>Bu makaleyi yazarken bircok kaynaktan, bloglardan, kendi bilgilerimden hatta ChatGPT'den dahi yararlandim. Daha derin okumalar yapmak isterseniz lutfen kaynaklar bolumune goz atin!</code></p>
<h3 id="tl-dr">TL; DR</h3>
<p>2022'de Vision Transformer (ViT), ÅŸu anda bilgisayar gÃ¶rÃ¼ÅŸÃ¼nde son teknoloji olan ve bu nedenle farklÄ± gÃ¶rÃ¼ntÃ¼ tanÄ±ma gÃ¶revlerinde yaygÄ±n olarak kullanÄ±lan evriÅŸimli sinir aÄŸlarÄ±na (CNN'ler) rekabetÃ§i bir alternatif olarak ortaya Ã§Ä±ktÄ±.</p>
<p>ViT modelleri, hesaplama verimliliÄŸi ve doÄŸruluÄŸu aÃ§Ä±sÄ±ndan mevcut en son teknolojiye (CNN) neredeyse 4 kat daha iyi performans gÃ¶steriyor.</p>
<p>Bu makale aÅŸaÄŸÄ±daki konulardan bahsedecegim:</p>
<ul>
<li>Vision Transformer (ViT) nedir?</li>
<li>GÃ¶rÃ¼ntÃ¼ TanÄ±ma'da ViT modellerini kullanma GÃ¶rÃ¼ntÃ¼ DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ler nasÄ±l Ã§alÄ±ÅŸÄ±r?</li>
<li>Vision Transformers'Ä±n KullanÄ±m DurumlarÄ± ve uygulamalarÄ±</li>
</ul>
<pre><code>- Devamindaki konulari yaz!
</code></pre>
<h2 id="vision-transformers">Vision Transformers</h2>
<p><img alt="vit" src="../../assets/vit.png" /></p>
<p>Uzun yillardir CNN algoritmalari goruntu isleme konularinda neredeyse tek cozumumuzdu. <a href="https://arxiv.org/abs/1512.03385">ResNet</a>, <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a>, <a href="https://arxiv.org/abs/1512.00567">Inception</a> vb. gibi tum mimariler temelde CNN mimarilerini kullanarak goruntu isleme problemlerimizi cozmede bize yardimci oluyordu. Bugun sizinle goruntu isleme konusunda farkli bir yaklasim olan ViT'ler yani Vision Transformerlari inceleyecegiz.</p>
<p>Aslinda Transformer kavrami NLP alaninda yurutulen teknolojiler icin ortaya kondu. <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> adiyla yayinlanan makale NLP problemlerinin cozumu icin devrimsel cozumler getirdi, artik Transformer-based mimarilar NLP gorevleri icin standart bir hale geldi.</p>
<p>Cok da uzun bir sure gecmeden dogal dil alaninda kullanilan bu mimari goruntu alaninda da ufak degisikliklerle uyarlandi. Bu calismayi <a href="https://arxiv.org/abs/2010.11929">An image is worth 16x16 words</a> olarak bu linkteki paperdan okuyabilirsiniz.</p>
<p>Asagida daha detayli anlatacagim fakat surec temel olarak bir goruntuyu 16x16 boyutlu parcalara ayirarak embeddinglerini cikartmak uzerine calisiyor. Temel bazi konulari anlatmadan bu mekanikleri aciklamak cok zor bu yuzden hiz kaybetmeden konuyu daha iyi anlamak icin alt basliklara gecelim.</p>
<h2 id="vits-vs-cnn">ViTs vs. CNN</h2>
<p>Bu iki mimariyi karsilastirdigimizda acik ara ViTlerin cok daha etkileyici oldugunu gorebiliyoruz.</p>
<p>Vision Transformerlar, training iÃ§in daha az hesaplama kaynaÄŸÄ± kullanirken ayni zamanda, evriÅŸimli sinir aÄŸlarÄ±na (CNN) daha iyi performans gosteriyor.</p>
<p>Birazdan asagida daha detayli olarak anlatacagim fakat temelde CNN'ler piksel dizilerini kullanÄ±r, ViT ise gÃ¶rÃ¼ntÃ¼leri sabit boyutlu ufak parcalara boler. Her bir parca transformer encoder ile patch, positional vs. embeddingleri cikartilir (asagida anlatacagim konular bunlari iceriyor). AyrÄ±ca ViT modelleri, hesaplama verimliliÄŸi ve doÄŸruluÄŸu sÃ¶z konusu olduÄŸunda CNN'lerden neredeyse dÃ¶rt kat daha iyi performans gÃ¶steriyorlar.</p>
<p>ViT'deki self-attention katmanÄ±, bilgilerin genel olarak gÃ¶rÃ¼ntÃ¼nÃ¼n tamamÄ±na yerleÅŸtirilmesini mÃ¼mkÃ¼n kiliyor bu demek oluyor ki yeniden birlestirmek istedigimizde veya yenilerini olusturmak istedigimizde bu bilgi de elimizde olacak, yani modele bunlari da ogretiyoruz.</p>
<p><img alt="attention map" src="../../assets/attention-map-vit.webp" /></p>
<p><em>Raw gorselleri (solda)  ViT-S/16 modeliyle attention haritalari (sagda)</em></p>
<p>Kaynak:  <em>When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations</em> <a href="https://arxiv.org/abs/2106.01548">https://arxiv.org/abs/2106.01548</a></p>
<h2 id="attention-mekanizmasi">Attention Mekanizmasi</h2>
<p><code>Bu bolum ChatGPT ile yazildi</code></p>
<p>Ã–zetle, NLP (DoÄŸal Dil Ä°ÅŸleme) iÃ§in geliÅŸtirilen attention(dikkat) mekanizmalarÄ±, yapay sinir aÄŸÄ± modellerinin girdiyi daha iyi iÅŸleyip anlamasÄ±na yardÄ±mcÄ± olmak iÃ§in kullanÄ±lÄ±r. Bu mekanizmalar, girdinin farklÄ± bÃ¶lÃ¼mlerine farklÄ± aÄŸÄ±rlÄ±k verilerek Ã§alÄ±ÅŸÄ±r, bu sayede model girdiyi iÅŸlerken belli bÃ¶lÃ¼mlere daha fazla dikkat eder.</p>
<p>Attention mekanizmalarÄ±, dot-product attention, multi-head attention ve transformer attention gibi farklÄ± tÃ¼rleri geliÅŸtirilmiÅŸtir. Bu mekanizmalar her birisi biraz farklÄ± ÅŸekilde Ã§alÄ±ÅŸsa da, hepsi girdinin farklÄ± bÃ¶lÃ¼mlerine farklÄ± aÄŸÄ±rlÄ±k verilerek modelin belli bÃ¶lÃ¼mlere daha fazla dikkat etmesine izin verme ilkesi Ã¼zerine Ã§alÄ±ÅŸÄ±r.</p>
<p>Ã–rneÄŸin, bir makine Ã§eviri gÃ¶revinde, bir attention mekanizmasÄ± modelin kaynak dil cÃ¼mlesindeki belli kelimeleri Ã¼reterek hedef dil cÃ¼mlesine dikkat etmesine izin verebilir. Bu, modelin daha doÄŸru Ã§eviriler Ã¼retebilmesine yardÄ±mcÄ± olur, Ã§Ã¼nkÃ¼ kaynak dil kelimelerinin anlam ve baÄŸlamÄ±nÄ± dikkate alarak Ã§eviri Ã¼retebilir.</p>
<p>Genel olarak, attention mekanizmalarÄ± birÃ§ok state-of-the-art NLP modelinin bir parÃ§asÄ±dÄ±r ve bu modellerin Ã§eÅŸitli gÃ¶revlerde performansÄ±nÄ± geliÅŸtirme konusunda Ã§ok etkili olduÄŸu gÃ¶sterilmiÅŸtir.</p>
<p><code>ChatGPT sonu</code></p>
<p>Aslinda konuya hakim biri icin daha iyi bir aciklama fakat cok kisa bu alanda hicbir bilgisi olmayan birisi icin basitlestirilmis bir aciklama yapma geregi duyuyorum.</p>
<p><code>ACIKLAMA GELECEK</code></p>
<h2 id="vit-implementasyonlari">ViT Implementasyonlari</h2>
<p>Fine-tune edilmis ve pre-trained  ViT modelleri <a href="https://github.com/google-research/">Google Research</a>'un Github'Ä±nda mevcut:</p>
<p><a href="https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a></p>
<p>Pytorch Implementasyonlari lucidrains'in Github reposunda bulabilirsiniz:</p>
<p><a href="https://github.com/lucidrains/vit-pytorch">https://github.com/lucidrains/vit-pytorch</a></p>
<p>Ayni zamanda <code>timm</code> kullanarak hazir modelleri hizlica kullanabilirsiniz.</p>
<p><a href="https://github.com/rwightman/pytorch-image-models">https://github.com/rwightman/pytorch-image-models</a></p>
<h2 id="mimari">Mimari</h2>
<p>ViT mimarisi birkaÃ§ aÅŸamadan oluÅŸuyor:</p>
<ol>
<li>
<p><strong>Patch + Position Embedding (inputs)</strong> - GiriÅŸ gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ bir dizi gÃ¶rÃ¼ntÃ¼ parcalarina (patches) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve parcalarin hangi sÄ±rayla geldiÄŸini bilmek icin bir konum numarasÄ± ekler.</p>
</li>
<li>
<p><strong>Linear projection of flattened patches (Embedded Patches)</strong> - GÃ¶rÃ¼ntÃ¼ parcalari embeddinglere donusturulur, gÃ¶rÃ¼ntÃ¼leri direkt kullanmak yerine embeddingleri kullanmanÄ±n yararÄ±, embeddingler gÃ¶rÃ¼ntÃ¼nÃ¼n eÄŸitimle Ã¶ÄŸrenilebilir bir temsili olmasÄ±dÄ±r.</p>
</li>
<li>
<p><strong>Norm</strong> - Bir sinir aÄŸÄ±nÄ± dÃ¼zenli hale getirmek (overfitting'i azaltmak) iÃ§in bir teknik olan "Layer Normalization" veya "LayerNorm"un kÄ±saltmasÄ±dÄ±r.</p>
</li>
<li>
<p><strong>Multi-Head Attention</strong> - Bu, Multi-Headed Self-Attention layer veya kÄ±saca "MSA" dÄ±r.</p>
</li>
<li>
<p><strong>MLP (Multilayer perceptron)</strong> - Genellikle herhangi bir feed-forward (ileri besleme) katmanÄ± koleksiyonu olarak dusunebilirsiniz.</p>
</li>
<li>
<p><strong>Transformer Encoder</strong> - Transformer Encoder, yukarÄ±da listelenen katmanlarÄ±n bir koleksiyonudur. Transformer Encoderin iÃ§inde iki skip (atlama) baÄŸlantÄ±sÄ± vardÄ±r ("+" sembolleri), katmanÄ±n girdilerinin doÄŸrudan sonraki katmanlarÄ±n yanÄ± sÄ±ra hemen sonraki katmanlara beslendiÄŸi anlamÄ±na gelir. Genel ViT mimarisi, birbiri Ã¼zerine yÄ±ÄŸÄ±lmÄ±ÅŸ bir dizi Transformer kodlayÄ±cÄ±dan oluÅŸur.</p>
</li>
<li>
<p><strong>MLP Head</strong> - Bu, mimarinin Ã§Ä±ktÄ± katmanÄ±dÄ±r, bir girdinin Ã¶ÄŸrenilen Ã¶zelliklerini bir sÄ±nÄ±f Ã§Ä±ktÄ±sÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rmasÄ± Ã¼zerinde Ã§alÄ±ÅŸtÄ±ÄŸÄ±mÄ±z iÃ§in buna "sÄ±nÄ±flandÄ±rÄ±cÄ± kafa" da diyebilirsiniz. MLP head yapÄ±sÄ± MLP bloÄŸuna benzer.</p>
</li>
</ol>
<h2 id="vit-mimarisi">ViT Mimarisi</h2>
<p><img alt="vit architecture" src="../../assets/vit-arch.png" /></p>
<h3 id="patch-embeddings">Patch Embeddings</h3>
<p>Standart Transformer, giriÅŸi tek boyutlu token embedding dizisi olarak alÄ±r. 2B gÃ¶rÃ¼ntÃ¼leri iÅŸlemek iÃ§in <strong>xâˆˆR^{HÃ—WÃ—C}</strong> gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ dÃ¼zleÅŸtirilmiÅŸ 2B patchlere (goruntu parcalarina) yeniden ÅŸekillendiriyoruz.</p>
<p>Burada, (H, W) orijinal gÃ¶rÃ¼ntÃ¼nÃ¼n Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼dÃ¼r ve (P, P) her gÃ¶rÃ¼ntÃ¼ parÃ§asÄ±nÄ±n Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼dÃ¼r. Resim sabit boyutlu parcalara bÃ¶lÃ¼nmÃ¼ÅŸtÃ¼r, aÅŸaÄŸÄ±daki resimde patch  (parca) boyutu 16Ã—16 olarak alÄ±nmÄ±ÅŸtÄ±r. Yani gÃ¶rÃ¼ntÃ¼nÃ¼n boyutlarÄ± 48Ã—48 olacaktÄ±r. (Cunku 3 kanal var)</p>
<p>Self-attention maliyeti quadratictir. GÃ¶rÃ¼ntÃ¼nÃ¼n her pikselini girdi olarak iletirsek, Self-attention her pikselin diÄŸer tÃ¼m piksellerle ilgilenmesini gerektirir. Self-attention ikinci dereceden maliyeti Ã§ok maliyetli olacak ve gerÃ§ekÃ§i girdi boyutuna Ã¶lÃ§eklenmeyecek; bu nedenle, gÃ¶rÃ¼ntÃ¼ parcalara bÃ¶lÃ¼nÃ¼r.</p>
<p>Yani sair burada her pikselle ugrasmak sonsuza kadar surecegi icin 16x16 boyutlu goruntu bolumlerinin embeddinglerini almanin parametre sayisini dusureceginden bahsediyor.</p>
<pre><code class="language-python">import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
</code></pre>
<pre><code class="language-python">img = Image.open('cobanov-profile.jpg')
img.thumbnail((224, 224))
array_img = np.array(img)
array_img.shape
</code></pre>
<pre><code class="language-python"># Setup hyperparameters and make sure img_size and patch_size are compatible
img_size = 224
patch_size = 16
num_patches = img_size/patch_size 
assert img_size % patch_size == 0, &quot;Image size must be divisible by patch size&quot; 
print(f&quot;Number of patches per row: {num_patches}\
        \nNumber of patches per column: {num_patches}\
        \nTotal patches: {num_patches*num_patches}\
        \nPatch size: {patch_size} pixels x {patch_size} pixels&quot;)

</code></pre>
<pre><code class="language-python"># Create a series of subplots
fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float
                        ncols=img_size // patch_size, 
                        figsize=(num_patches, num_patches),
                        sharex=True,
                        sharey=True)

# Loop through height and width of image
for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height
    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width

        # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))
        axs[i, j].imshow(array_img[patch_height:patch_height+patch_size, # iterate through height 
                                        patch_width:patch_width+patch_size, # iterate through width
                                        :]) # get all color channels

        # Set up label information, remove the ticks for clarity and set labels to outside
        axs[i, j].set_ylabel(i+1, 
                             rotation=&quot;horizontal&quot;, 
                             horizontalalignment=&quot;right&quot;, 
                             verticalalignment=&quot;center&quot;) 
        axs[i, j].set_xlabel(j+1) 
        axs[i, j].set_xticks([])
        axs[i, j].set_yticks([])
        axs[i, j].label_outer()

# Set a super title
plt.show()
</code></pre>
<p><img alt="patch-embeddings" src="../../assets/patch_emb.png" /></p>
<h2 id="linear-projection-of-flattened-patches">Linear Projection of Flattened Patches</h2>
<p>Parcalari Transformer bloÄŸuna geÃ§irmeden Ã¶nce, makalenin yazarlarÄ± yamalarÄ± Ã¶nce doÄŸrusal bir projeksiyondan geÃ§irmeyi faydalÄ± bulmuslar. Bir yamayÄ± alÄ±p bÃ¼yÃ¼k bir vektÃ¶re aÃ§arlar ve patch embeddingler (goruntu parcalarinin gommeleri? veya embedddingleri) oluÅŸturmak iÃ§in embedding matrisiyle Ã§arparlar ve bu konumsal gÃ¶mmeyle (positional embeddings) birlikte transformatÃ¶re giden ÅŸeydir.</p>
<p>Her goruntu parcasi (patches), tÃ¼m piksel kanallarÄ±nÄ± bir yamada birleÅŸtirerek ve ardÄ±ndan bunu doÄŸrusal olarak istenen giriÅŸ boyutuna yansÄ±tarak embed edilen bir 1B patch'e dÃ¼zleÅŸtirilir.</p>
<p>Ne demek istedigimi bu gorselde cok daha iyi anlayacaginizi dusunuyorum.
<img alt="linear embeddings" src="../../assets/linear-projection.png" /></p>
<p><em>Kaynak: <a href="https://arxiv.org/pdf/2103.14803.pdf">Face Transformer for Recognition</a></em></p>
<h2 id="positional-embeddings">Positional embeddings</h2>
<p>Nasil konusurken dilde kelimelerin sÄ±rasÄ± kurdugunuz cumlenin anlamini tamamen degistiriyorsa, goruntuler uzerinde de buna dikkat etmek gerekiyor. Maalesef transformerlar, patch embeddinglerin "sÄ±rasÄ±nÄ±" dikkate alan herhangi bir varsayÄ±lan mekanizmaya sahip deÄŸiller.</p>
<p>Bir yapboz yaptiginizi dusunun, elinizdeki parcalar (yani onceki adimlarda yaptigimiz patch embeddingler) karisik bir duzende geldiginde goruntunun tamaminda ne oldugunu anlamak oldukca zordur, bu transformerlar iÃ§in de geÃ§erli. Modelin yapboz parÃ§alarÄ±nÄ±n sÄ±rasÄ±nÄ± veya konumunu Ã§Ä±karmasÄ±nÄ± saÄŸlamanÄ±n bir yoluna ihtiyacÄ±mÄ±z var.</p>
<p>Transformerlar, giriÅŸ elemanlarÄ±nÄ±n yapÄ±sÄ±ndan baÄŸÄ±msÄ±zdÄ±r. Her yamaya Ã¶ÄŸrenilebilir positional embeddings (konum yerleÅŸtirmeleri) eklemek, modelin gÃ¶rÃ¼ntÃ¼nÃ¼n yapÄ±sÄ± hakkÄ±nda bilgi edinmesine olanak tanÄ±r.</p>
<p>Positional embeddingler de, bu dÃ¼zeni modele aktarmamizi sagliyor. ViT iÃ§in, bu positional embeddingler, patch embeddingler ile aynÄ± boyutluluÄŸa sahip Ã¶ÄŸrenilmiÅŸ vektÃ¶rlerdir.</p>
<p>Bu positional embeddingler, eÄŸitim sÄ±rasÄ±nda ve (bazen) ince ayar sÄ±rasÄ±nda Ã¶ÄŸrenilir. EÄŸitim sÄ±rasÄ±nda, bu embeddingler, Ã¶zellikle aynÄ± sÃ¼tunu ve satÄ±rÄ± paylaÅŸan komÅŸu konum yerleÅŸtirmelerine yÃ¼ksek benzerlik gÃ¶sterdikleri vektÃ¶r uzaylarÄ±nda birleÅŸir.</p>
<p><img alt="" src="../../assets/visualizing-positional-encodings-vit.png" /></p>
<h2 id="transformer-encoding">Transformer Encoding</h2>
<ul>
<li>
<p><strong>Multi-Head Self Attention Layer(MSP)</strong> birden fazla attention ciktisini lineer olarak beklenen boyutlara esitlemek iÃ§in kullanilir. MSP, gÃ¶rÃ¼ntÃ¼deki yerel ve global baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenmeye yardÄ±mcÄ± olur.</p>
</li>
<li>
<p><strong>Multi-Layer Perceptrons(MLP)</strong> - Klasik sinir agi katmani fakat aktivasyon fonksiyonu olarak GELU <a href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units</a> kullaniyoruz.</p>
</li>
<li>
<p><strong>Layer Norm(LN)</strong> eÄŸitim gÃ¶rÃ¼ntÃ¼leri arasÄ±nda herhangi bir yeni baÄŸÄ±mlÄ±lÄ±k getirmediÄŸinden her bloktan Ã¶nce uygulanÄ±r. EÄŸitim sÃ¼resini ve genelleme performansÄ±nÄ± iyileÅŸtirmeye yardÄ±mcÄ± olur. Burada Misra'nin harika bir <a href="https://www.youtube.com/watch?v=2V3Uduw1zwQ&amp;ab_channel=AssemblyAI">videosu</a> var. Bu da <a href="https://arxiv.org/abs/1607.06450">Paper</a>.</p>
</li>
<li>
<p><strong>Residual connections</strong> gradyanlarÄ±n doÄŸrusal olmayan aktivasyonlardan geÃ§meden doÄŸrudan aÄŸ Ã¼zerinden akmasÄ±na izin verdiÄŸi iÃ§in her bloktan sonra uygulanÄ±r.
Image classification iÃ§in, Ã¶n eÄŸitim zamanÄ±nda bir hidden layer ve fine-tuning iÃ§in tek bir linear layer ile MLP kullanÄ±larak bir classification head uygulanÄ±r. ViT'nin Ã¼st katmanlarÄ± global Ã¶zellikleri Ã¶ÄŸrenirken, alt katmanlar hem global hem de yerel Ã¶zellikleri Ã¶ÄŸrenir. Bu da aslinda ViT'nin daha genel kalÄ±plarÄ± Ã¶ÄŸrenmesini sagliyor.</p>
</li>
</ul>
<h2 id="toparlama">Toparlama</h2>
<p>Evet oldukca fazla terim, teori ve mimari inceledik kafalarda daha iyi oturtmak adina bu gif surecin nasil isledigini guzel bir sekilde ozetliyor.</p>
<p><img alt="vit gif" src="../../assets/vit-fig.gif" /></p>
<h2 id="egitim">Egitim</h2>
<p><code>Bolum yeniden yazilacak</code></p>
<p>ViT, bÃ¼yÃ¼k veri kÃ¼melerinde Ã¶nceden eÄŸitilmiÅŸtir ve daha kÃ¼Ã§Ã¼k bir veri kÃ¼mesine ince ayar yapÄ±lmÄ±ÅŸtÄ±r.</p>
<p>Ä°nce ayar yapÄ±lÄ±rken, Ã¶nceden eÄŸitilmiÅŸ son tahmin kafasÄ± kaldÄ±rÄ±lÄ±r ve daha kÃ¼Ã§Ã¼k veri kÃ¼mesine dayalÄ± olarak sÄ±nÄ±flarÄ± tahmin etmek iÃ§in sÄ±fÄ±r baÅŸlatÄ±lmÄ±ÅŸ bir ileri besleme katmanÄ± ekleriz.</p>
<p>Ä°nce ayar, modelin Ã¶nceden eÄŸitildiÄŸinden daha yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ bir gÃ¶rÃ¼ntÃ¼ye uygulanabilir, ancak yama boyutu aynÄ± kalmalÄ±dÄ±r.</p>
<p>Transformers, gÃ¶rÃ¼ntÃ¼ yapÄ±sÄ± hakkÄ±nda Ã¶nceden bilgiye sahip deÄŸildir ve bu nedenle daha uzun eÄŸitim sÃ¼relerine sahiptir ve modeli eÄŸitmek iÃ§in bÃ¼yÃ¼k veri kÃ¼meleri gerektirir.</p>
<h2 id="usage">Usage</h2>
<p>Eger basitce bir ViT kullanmak isterseniz bunun icin ufacik bir rehberi de buraya ekliyorum.</p>
<p>Muhtemelen denk gelmissinizdir artik yapay zeka alaninda yeni bir sey ciktiginda birakin bunun implementasyonunu kullanimini da birkac satir koda indirmek moda oldu.</p>
<p>Yukarida anlattigim her seyi birkac satir Python koduyla nasil yapildigina bakalim.</p>
<p>Colab linki: <a href="https://colab.research.google.com/drive/1sPafxIo6s1BBjHbl9e0b_DYGlb2AMBC3?usp=sharing">https://colab.research.google.com/drive/1sPafxIo6s1BBjHbl9e0b_DYGlb2AMBC3?usp=sharing</a></p>
<p>Oncelikle pretrained model instantiate <em>(orneklendirmek?)</em> edelim.</p>
<pre><code class="language-python">import timm

model = timm.create_model('vit_base_patch16_224', pretrained=True)
model.eval()
</code></pre>
<p>Goruntumuzu yukleyip on islemelerini tamamlayim. Ben burada twitter profil fotograimi kullanacagim.</p>
<p><img alt="" src="../../assets/mert.jpg" /></p>
<pre><code class="language-python"># eger kendiniz bir gorsel vermek isterseniz 
# asagidaki kodda bu kismi comment'e alip, 
# filename kismina local path verebilirsiniz.

url, filename = (&quot;https://github.com/pytorch/hub/raw/master/images/dog.jpg&quot;, &quot;dog.jpg&quot;)
urllib.request.urlretrieve(url, filename)
</code></pre>
<pre><code class="language-python">import urllib
from PIL import Image
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

config = resolve_data_config({}, model=model)
transform = create_transform(**config)

url, filename = (&quot;https://pbs.twimg.com/profile_images/1594739904642154497/-7kZ3Sf3_400x400.jpg&quot;, &quot;mert.jpg&quot;)
urllib.request.urlretrieve(url, filename)
img = Image.open(filename).convert('RGB')
tensor = transform(img).unsqueeze(0) # transform and add batch dimension

</code></pre>
<p>Tahminleri alalim</p>
<pre><code class="language-python">import torch

with torch.no_grad():
    out = model(tensor)
probabilities = torch.nn.functional.softmax(out[0], dim=0)
print(probabilities.shape)

# prints: torch.Size([1000])
</code></pre>
<p>En populer 5 tahminin siniflarina bakalim.</p>
<pre><code class="language-python"># Get imagenet class mappings
url, filename = (&quot;https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt&quot;, &quot;imagenet_classes.txt&quot;)
urllib.request.urlretrieve(url, filename) 
with open(&quot;imagenet_classes.txt&quot;, &quot;r&quot;) as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
</code></pre>
<pre><code class="language-python"># prints class names and probabilities like:

# trench coat 0.422695130109787
# bulletproof vest 0.18995067477226257
# suit 0.06873432546854019
# sunglasses 0.02222270704805851
# sunglass 0.020680639892816544
</code></pre>
<h2 id="sources">Sources</h2>
<ul>
<li><a href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview">https://www.learnpytorch.io/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview</a></li>
<li><a href="https://theaisummer.com/vision-transformer/">https://theaisummer.com/vision-transformer/</a></li>
<li><a href="https://medium.com/swlh/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2">https://medium.com/swlh/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2</a></li>
<li><a href="https://viso.ai/deep-learning/vision-transformer-vit/">https://viso.ai/deep-learning/vision-transformer-vit/</a></li>
<li><a href="https://arxiv.org/abs/2106.01548">https://arxiv.org/abs/2106.01548</a></li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../ldm/" class="btn btn-neutral float-left" title="What is Latent Diffusion"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../glossary/" class="btn btn-neutral float-right" title="Glossary">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../ldm/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../glossary/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
